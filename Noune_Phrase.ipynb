{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1='Today is a very great day.'\n",
    "x2='Indian politicians are very corrupt.'\n",
    "x3='Many buildings were destroyed in the earthquake'\n",
    "x4='Fastest flight from Bangalore to Delhi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Today', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('very', 'RB'),\n",
       " ('great', 'JJ'),\n",
       " ('day', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=nltk.pos_tag(nltk.word_tokenize(x1))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noune Phrase :- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule says that an NP chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN). Using this grammar, we create a chunk parser-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk Grammar for noune phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r'''\n",
    "    NP: {<DT>?<JJ|JJS>*<NN|NNS>}\n",
    "        \n",
    "    '''\n",
    "cp=nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the tree using ChunkParser generated by the grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = cp.parse(tokens)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etract the 'Noun Phrase' from the generated tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today', 'great day']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_phrase=[]\n",
    "for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "    n_p= ' '.join([w for w, t in subtree.leaves()])\n",
    "    n_phrase.append(n_p)\n",
    "n_phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for Extracting Noun Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_noune_phrase(x):\n",
    "    tokens=nltk.pos_tag(nltk.word_tokenize(x))\n",
    "    grammar = r'''\n",
    "    NP: {<DT>?<JJ|JJS>*<NN|NNS>}\n",
    "        \n",
    "    '''\n",
    "    cp=nltk.RegexpParser(grammar)\n",
    "    tree = cp.parse(tokens)\n",
    "    n_phrase=[]\n",
    "    for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "        n_p= ' '.join([w for w, t in subtree.leaves()])\n",
    "        n_phrase.append(n_p)\n",
    "    n_phrase\n",
    "    \n",
    "    return n_phrase\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today', 'great day']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_noune_phrase(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Indian politicians']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_noune_phrase(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Many buildings', 'the earthquake']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_noune_phrase(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fastest flight']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_noune_phrase(x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
